{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c819b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f075e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileLocation = \"s3://cpsc5330s21/data-input/flight-data/2015-summary.csv\"\n",
    "flightData2015 = spark.read.option(\"inferScheme\", \"true\").option(\"header\", \"true\").csv(fileLocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd9a3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(flightData2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5019b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add7f8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081f8835",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(flightData2015.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204040df",
   "metadata": {},
   "source": [
    "### Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d0a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfbf3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13f36ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, FloatType, IntegerType\n",
    "\n",
    "flightDataSchema = StructType([StructField('dest_country_name', StringType(), nullable=False),\n",
    "                             StructField('origin_country_name', StringType(), nullable=False),\n",
    "                             StructField('cnt', LongType(), nullable=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e920115",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flightData2015 = spark.read.option(\"inferScheme\", \"true\").option(\"header\", \"true\").csv(fileLocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e8d539",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").schema(flightDataSchema).option(\"header\", \"true\").load(fileLocation)\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffa70b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b8f864",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e80ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f44304",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3215463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43423c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.take(1)[0]['dest_country_name'])\n",
    "print(df.take(1)[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b732ac99",
   "metadata": {},
   "source": [
    "### Other Ways to Create Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54685cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a list of rows -- also, nested schema\n",
    "from pyspark.sql.types import Row\n",
    "nameSchema = StructType([StructField('first', StringType()),\n",
    "                         StructField('last', StringType())])\n",
    "\n",
    "nameIDSchema = StructType([StructField('name', nameSchema), StructField('age', LongType())])\n",
    "\n",
    "r1 = Row(('manny', 'pep'), 10)\n",
    "r2 = Row(('moe', 'pep'), 20)\n",
    "\n",
    "nameIDdf = spark.createDataFrame([r1, r2], nameIDSchema)\n",
    "nameIDdf.show(2)\n",
    "nameIDdf.select('name.first').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9392b40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From an RDD\n",
    "from pyspark.sql import SQLContext\n",
    "textcorporaLocation = 's3://cpsc5330s21/data-input/textcorpora/'\n",
    "f = sc.wholeTextFiles(textcorporaLocation) \n",
    "sq = SQLContext(sc)\n",
    "documents = sq.createDataFrame(f, StructType([StructField('filename', StringType()), StructField('document', StringType())]))\n",
    "print(documents.count())\n",
    "documents.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee1a46b",
   "metadata": {},
   "source": [
    "### Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f7cc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Pandas we would expect \n",
    "print(df.cnt)\n",
    "print(df['cnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb047eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "print(col('count'))\n",
    "print(type(col('count')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b31330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns can be used to form expressions\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "print(type(col('cnt') + 5))\n",
    "print(type(sum('cnt')))\n",
    "print(type(col('dest_country_name') > col('origin_country_name')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b23a00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "print(df.select(sum(col('cnt'))))\n",
    "s = df.select(sum(col('cnt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc588622",
   "metadata": {},
   "outputs": [],
   "source": [
    "s.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac579ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "s.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3470aa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(sum('cnt')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1582692",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(sum('cnt')).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ac26e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(sum(col('cnt') * 3)).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ac3933",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.select(sum(col('count') * 3)).collect()[0]['sum((count * 3))'])\n",
    "print(df.select(sum(col('count') * 3)).collect()[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5000b289",
   "metadata": {},
   "source": [
    "### Select "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68805d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4602fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "df.select(expr(\"dest_country_name AS dcn\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a48c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(expr(\"*\"), expr(\"dest_country_name = origin_country_name as withinCountry\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1ceb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"dest_country_name\").withColumnRenamed(\"dest_country_name\", \"dcn\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f3428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09158d86",
   "metadata": {},
   "source": [
    "### SQL Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1f89bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"dftable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4612d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT DEST_COUNTRY_NAME, count(*) FROM dftable GROUP BY DEST_COUNTRY_NAME ORDER BY count(*) desc\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896c4dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrameWay =   df.groupBy(\"DEST_COUNTRY_NAME\").count()\n",
    "sqlWay       =   spark.sql(\"\"\"SELECT DEST_COUNTRY_NAME, count(*) FROM dftable GROUP BY DEST_COUNTRY_NAME\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32007d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sqlWay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a07d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrameWay.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebea9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a0f05b",
   "metadata": {},
   "source": [
    "### Filtering Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66338865",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(col(\"cnt\") > 1).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14b39d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"dest_country_name\", \"origin_country_name\").\\\n",
    "    filter(col(\"cnt\") > 1).\\\n",
    "    filter(col(\"dest_country_name\") != \"United States\").\\\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d14be78",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = df.select(\"dest_country_name\", \"origin_country_name\").filter(col(\"cnt\") > 1).filter(col(\"dest_country_name\") != \"United States\")\n",
    "sql = spark.sql(\"\"\"SELECT dest_country_name, origin_country_name from dftable where cnt > 1 and dest_country_name != 'United States'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d4ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.explain()\n",
    "sql.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5422b1a8",
   "metadata": {},
   "source": [
    "### Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a022ee76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.orderBy(col(\"dest_country_name\").asc(), col(\"cnt\").desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b45f318",
   "metadata": {},
   "source": [
    "### UDFs and Broadcast Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b45558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, column\n",
    "\n",
    "def shorten(country):\n",
    "    if len(country) < 3:\n",
    "        return country\n",
    "    else:\n",
    "        return country[0:3]\n",
    "\n",
    "shortenUDF = udf(lambda c: shorten(c), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010548fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"shortdest\", shortenUDF(col(\"dest_country_name\"))).drop('dest_country_name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a4442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "countryAbbrevs = {'Algeria': 'ALG', \n",
    "                  'Angola': 'ANGO', \n",
    "                  'Anguilla': 'ANGU', \n",
    "                  'Antigua and Barbuda': 'AAB', \n",
    "                  'United States': 'US'}\n",
    "caBroadcast = sc.broadcast(countryAbbrevs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b31a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(caBroadcast)\n",
    "print(caBroadcast.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450a9010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbreviate(country):\n",
    "    return caBroadcast.value.get(country, country)\n",
    " \n",
    "abbreviateUDF = udf(lambda c: abbreviate(c), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1727184",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"abbrevdest\", abbreviateUDF(col(\"dest_country_name\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0545986e",
   "metadata": {},
   "source": [
    "### Grouping and Aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1f2ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1c5bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('origin_country_name', 'cnt').groupBy('origin_country_name').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455297cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('origin_country_name', 'cnt').groupBy('origin_country_name').sum().orderBy(col(\"sum(cnt)\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b0e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_set\n",
    "cset = df.select('origin_country_name', 'dest_country_name').\\\n",
    "        groupBy(col('origin_country_name')).\\\n",
    "        agg(collect_set('dest_country_name'))\n",
    "cset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bed6bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cset.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cac2e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "cset.filter(col('origin_country_name') == 'United States').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df0333",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cset.filter(col('origin_country_name') == 'United States').collect()[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41ac93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfgrouped = df.select('origin_country_name', 'dest_country_name').\\\n",
    "    groupBy(col('origin_country_name')).\\\n",
    "    agg(collect_set('dest_country_name')).\\\n",
    "    withColumnRenamed('collect_set(dest_country_name)', 'dests').\\\n",
    "    filter(col('origin_country_name') == 'United States')\n",
    "dfgrouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c3d068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "exploded = dfgrouped.withColumn('exploded', explode(col('dests'))).drop('dests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becb7d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbb358c",
   "metadata": {},
   "source": [
    "### Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93756f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfFileLocation = 's3://cpsc5330s21/data-input/tfidf/tfidf.tsv'\n",
    "tfidfSchema = StructType([StructField('docid', StringType()),\n",
    "                          StructField('term', StringType()),\n",
    "                          StructField('tfidf', FloatType())])\n",
    "\n",
    "tfidf = spark.read.format(\"csv\").schema(tfidfSchema).option('header', False).option('delimiter', \"\\t\").load(tfidfFileLocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7a4ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b34983a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSchema = StructType([StructField('term', StringType()),\n",
    "                       StructField('df', IntegerType())])\n",
    "\n",
    "dfData = [Row(\"freak\", 2), Row(\"free\", 10), Row(\"freed\", 3), Row(\"freedom\", 5), Row(\"freeze\", 1)]\n",
    "docFreq = spark.createDataFrame(dfData, dfSchema)\n",
    "docFreq.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a9b763",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = tfidf.join(docFreq, tfidf[\"term\"] == docFreq[\"term\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbead3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "j.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797ff578",
   "metadata": {},
   "outputs": [],
   "source": [
    "j.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7136a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = docFreq.withColumnRenamed(\"term\", \"term2\")\n",
    "j2 = tfidf.join(df2, tfidf[\"term\"] == df2[\"term2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1035b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "j2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4228c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "j3 = j2.drop(\"term2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ab983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "j3.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae996b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "j4 = tfidf.join(docFreq, \"term\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c86559",
   "metadata": {},
   "outputs": [],
   "source": [
    "j4.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d7e5e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
