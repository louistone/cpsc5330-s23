{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3cad7123-c888-48a6-81ae-91543e02d59c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Lab 8: Spark Streaming For Log Processing\n",
    "\n",
    "This is a simple exercise in log processing.  The log files come from various servers at various time points.\n",
    "Each record in a log file is of the form ```serverID,severity,timestamp```, where  \n",
    "    - `serverID` is a string unique to the server  \n",
    "    - `severity` is a value of 2 (referred to as `SEV2` that represents no error, just a service call),  1 (referred to as `SEV1` that represents a minor error), or 0 (referred to as `SEV0` that represents a fatal/severe error)    \n",
    "    - `timestamp` is an integer starting at 1 (bigger numbers mean later)  \n",
    "\n",
    "For this lab, the four log files (on Canvas and Teams) will be \"delivered\" by being placed in an S3 bucket, for example `s3://spark-bucket-week9/LogDataLive/`.\n",
    "There are two servers in the log files, `s1` and `s2`, and the log records range from `t1` to `t10`.  \n",
    "The files are delivered with one file per server for five time units. For example, the file `s115.csv` has records for server `s1` for `t1` to `t5`.\n",
    "\n",
    "You want to process these new records incrementally, and are interested in these two \"reports\":\n",
    "\n",
    "1. The *volume report*: reports the number of `SEV2` events divided by the number of time units for each server. The number of time units for our purposes is `max(timestamp) - min(timestamp) + 1`. This volume report will not be cumulative, i.e., every time new log data comes in, the mapping from the server to `SEV2` events is updated  \n",
    "2. The *SEV0 log*: this is a sequence of records of the form ```serverID timestamp``` recording the timestamp of a `SEV0` event reported by a server. This report grows over time, i.e., each time a new log file is processed, new records are appended to the end.\n",
    "\n",
    "Your final results should be produced by two streaming queries:\n",
    "1. One that *modifies* the `SEV2` volume report, which is stored in memory\n",
    "2. One that *appends* to the `SEV0` log report, which is stored as a csv file in your S3 bucket\n",
    "\n",
    "### Submission\n",
    "There are two files (not a zip file) to submit in this lab: \n",
    "1. A retrospective report in a file `retrospective.pdf`: a reflection on the assignment, with the following components   \n",
    "    a. Your name   \n",
    "    b. How much time you spent on the assignment   \n",
    "    c. Were there aspects of the assignment that were particularly challenging? Particularly confusing?     \n",
    "    d. What were the main learning takeaways from this lab â€“ that is, did it introduce particular concepts or techniques that might help you as an analyst or engineer in the future?   \n",
    "2. This notebook file `lab8-YOURNAME.ipynb`   \n",
    "    a. make sure the output is also saved when saving and downloading your notebook  \n",
    "    b. make sure your results are copied to the last four cells of this notebook  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2e0b85e3-2fe4-4cec-8f56-2a9be19d3b56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Make sure (1) you have uploaded those 4 logs files to your S3 folder 'LogData'\n",
    "#           (2) you have created an empty S3 folder 'LogDataLive' (for simulating log streams)\n",
    "#           (3) you have created an empty S3 folder 'Lab9Output' (for saving your results)\n",
    "#           (4) replace the following URIs with yours\n",
    "s3_log_data_uri      = #'s3://spark-bucket-week9/LogData/'\n",
    "s3_log_data_live_uri = #'s3://spark-bucket-week9/LogDataLive/'\n",
    "s3_lab9_output_uri   = #'s3://spark-bucket-week9/Lab9Output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Did you check the comment above?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "34ff72b0-61d4-4e92-b478-c0c84dc9c375",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the schema for the log files based on the above description of the data \n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n",
    "\n",
    "logSchema = ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a513f133-14fd-4e46-bba8-855f401aadc0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the streaming DataFrame (readStream) on your log directory, using the schema you just created\n",
    "streamingLogData = spark.readStream.??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Get the SEV2 volume report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1e4dd595-b686-4ba0-9baf-3326c63995f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the data frame you just created to create another data frame with the \n",
    "# sev2 volume report.  It should have columns 'serverID' and 'avgVolume'\n",
    "\n",
    "from pyspark.sql.functions import ?? import functions your needed\n",
    "\n",
    "volumeReportDataFrame = streamingLogData..??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ad420b2b-d748-4c5b-b568-5369c44c9396",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create and start a query (writeStream) that generates the sev2 report;  it is an in-memory sink.\n",
    "volumeReportQuery = volumeReportDataFrame.??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5a44bb60-19c9-4f50-9278-641fb4806ca2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write a (very simple) spark SQL query to show the contents of your query. It should initially be empty\n",
    "spark.sql??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT EDIT THIS CELL\n",
    "\n",
    "# Helper functions for moving files in S3\n",
    "\n",
    "import boto\n",
    "\n",
    "# delete all the files in a folder\n",
    "def empty_s3_folder(bucket_name = 'spark-bucket-week9', folder_path = 'LogDataLive/'):\n",
    "    # establish a connection to S3\n",
    "    conn = boto.connect_s3(host='s3.amazonaws.com')\n",
    "    s3_bucket = conn.get_bucket(bucket_name)\n",
    "\n",
    "    # iterate through the objects in the folder\n",
    "    for key in s3_bucket.list(prefix=folder_path):\n",
    "        if not str(key).endswith('/>'):\n",
    "            key.delete()\n",
    "        \n",
    "    print(f'All files in the {folder_path} folder are removed')\n",
    "    \n",
    "\n",
    "# copy file from one folder to live folder; simulating live data stream\n",
    "def copy_s3_file_to_live_folder(log_file, bucket_name='spark-bucket-week9'):\n",
    "    # establish a connection to S3\n",
    "    conn = boto.connect_s3(host='s3.amazonaws.com')\n",
    "    \n",
    "    # remember to pass in your bucket name\n",
    "    my_bucket = conn.get_bucket(bucket_name) \n",
    "    # make sure you have these folders\n",
    "    src_folder = 'LogData/'\n",
    "    dst_folder = 'LogDataLive/'\n",
    "    \n",
    "    # copy from one folder to another folder of the same bucket\n",
    "    my_bucket.copy_key(dst_folder + log_file, bucket_name, src_folder  + log_file)\n",
    "    print(f'Copied {log_file} to {dst_folder}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First make sure the \"live data\" folder is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6cd1418b-7ed3-4713-a5b7-7f8162fbfa2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Copy two log files from t1 to t5 into your 'LogDataLive' folder for both servers \n",
    "copy_s3_file_to_live_folder(??)\n",
    "copy_s3_file_to_live_folder(??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f245ecc8-1a90-4a36-9c2d-c1d25b5a2827",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rerun the query to show that the sev2 volume report has been updated (wait a while)\n",
    "spark.sql??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d0589690-9a47-49ac-b303-44e6fad7da49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Copy two log files from t6 to t10 into your 'LogDataLive' folder for both servers \n",
    "copy_s3_file_to_live_folder(??)\n",
    "copy_s3_file_to_live_folder(??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c4246b60-8de0-4484-b6f3-274eb1ca3ad5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run the query again to verify that the report was updated. \n",
    "# Be sure to wait for a little while to make sure the query is updated.\n",
    "spark.sql??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8b8e2417-e879-45bf-a99e-bedb89fd9e81",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Part 2. Get the SEV0 log report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all files from your \"live\" directory before working on this part\n",
    "empty_s3_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "744e269d-0f08-4634-aca5-bd9d4aac8741",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a data frame on top of your original data frame that holds the raw data, \n",
    "# this data frame for the sev0 report is just <serverID> <time stamp> ordered by timestamp, \n",
    "# and by server ID within timestamp\n",
    "\n",
    "sev0 = streamingLogData.??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4e9bb771-f5ba-4295-be8a-061363ab8eb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a query on your sev0 data frame that writes the table to a csv file, \n",
    "sev0SaveQuery = sev0.??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4aa8b252-8528-43ae-a8b6-0efa07c2e54b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Copy two files into your 'LogDataLive' folder for both servers for time period 1 through 5\n",
    "copy_s3_file_to_live_folder(??)\n",
    "copy_s3_file_to_live_folder(??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above log stream will cause our streaming job to produce some results to our S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy two files into your 'LogDataLive' folder for both servers for time period 6 through 10\n",
    "copy_s3_file_to_live_folder(??)\n",
    "copy_s3_file_to_live_folder(??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above log stream will *again* cause our streaming job to produce some results to our S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "edc2cd6e-64f8-4a43-b505-ef7c8b27955e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now you're done with the lab\n",
    "# clean up / stop all running streaming jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put Your Results Here\n",
    "There should be four groups of numbers produced from the above two parts. For easy grading, copy your results into the following cells though we will run your notebook.   \n",
    "\n",
    "**Important Note**: Make sure your notebook can be executed from beginning to end without error. You should check that before you hand it in. Simply putting results into a non-working notebook will not be considered as a valid submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part 1, after you streamed the first two log files, `s115.csv` and `s215.csv`, \n",
    "what is the produced volume of `SEV2` on each server?   Put your output in this Markdown cell, and wrap it in <pre> tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part 1, after you streamed the last two log files, `s1610.csv` and `s2610.csv`, what is the\n",
    "produced volume of `SEV2` on each server?   Put your output in this Markdown cell, and wrap it in <pre> tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part 2, after you streamed the first two log files, `s115.csv` and `s215.csv`, what is the current `SEV0` log?\n",
    "  Put your output in this Markdown cell, and wrap it in <pre> tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part 2, after you streamed the last two log files, `s1610.csv` and `s2610.csv`, what is the current `SEV0` log?   Put your output in this Markdown cell, and wrap it in <pre> tag."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "StreamingLab",
   "notebookOrigID": 3258061014311749,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "name": "StreamingLab",
  "notebookId": 3750749627479206
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
