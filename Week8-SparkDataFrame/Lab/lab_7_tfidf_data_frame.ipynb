{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad16dece",
   "metadata": {},
   "source": [
    "### Important Note on EMR Version\n",
    "Version must be 6.5.0 or this code will not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adfef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, FloatType, IntegerType, Row\n",
    "from pyspark.sql.functions import udf, col, column\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d6cfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these to point to your own S3 locations\n",
    "#   The first is a directory containing the text corpus\n",
    "#   The second is a file containing the stopwords, one per line\n",
    "\n",
    "books_directory = 's3://5330books/'\n",
    "stopwords_file = 's3://cpsc5330s23/stopwords.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bca82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stopwords into a set, used by termify()\n",
    "\n",
    "stopwords = set()\n",
    "for word in sc.textFile(stopwords_file).collect():\n",
    "    stopwords.add(word)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74635e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same versions of termify and get_docid as previous labs\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "def termify(line):\n",
    "    terms = []\n",
    "    words = re.findall(r'[^\\W_]+', line)\n",
    "    for word in words:\n",
    "        lowered = word.lower()\n",
    "        if (len(lowered) > 1) and (lowered not in stopwords) and (not re.search(r'^\\d*$', lowered)):\n",
    "            terms.append(lowered)\n",
    "    return terms\n",
    "\n",
    "def get_docid(filepath):\n",
    "    return filepath.split('/')[-1][: -4]\n",
    "\n",
    "# Spark SQL requires user-defined functions (UDFs).  Create a UDF from a function.\n",
    "# Spark also needs to know the return type -- termify returns an array of strings\n",
    "\n",
    "getDocidUDF = udf(lambda f: get_docid(f), StringType())\n",
    "termifyUDF = udf(lambda t: termify(t), ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288315fa",
   "metadata": {},
   "source": [
    "#### The Indexing Phase\n",
    "\n",
    "All operations are packaged together into a single function, indexDocuments\n",
    "*  Accepts:  directory containing the text documents\n",
    "*  Returns:  a Data Frame with columns (term, docid, tfidf) and schema (string, string, float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbcd821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexDocuments(path):\n",
    "    # Create RDD to read the text files (filepath, full_document_text)\n",
    "    f = ????\n",
    "    \n",
    "    # Get the SQLContext to create a data frame from an RDD.  \n",
    "    sq = SQLContext(sc)\n",
    "    \n",
    "    # The dataframe will have columns 'filename' and 'document', both are strings\n",
    "    documents = sq.createDataFrame(?rdd?, ?schema?)\n",
    "    \n",
    "    # Run termify udf on the document.  Produces a new column 'terms' which is a list of strings\n",
    "    d2 = ???\n",
    "    \n",
    "    # Explode the terms to get one row per term.  New data frame has columns 'document' and 'term'\n",
    "    d3 = ???\n",
    "    \n",
    "    # Convert the 'filename' column to a 'docid'.  Drop the old column 'filename'\n",
    "    d4 = d3.withColumn('docid', getDocidUDF(col('filename'))).drop('filename')\n",
    "    \n",
    "    # Term frequency is a data frame where we group by 'docid' and 'term' and count within each group.\n",
    "    # Its columns are (docid, term, count)\n",
    "    tf = ??\n",
    "    \n",
    "    # Doc Size is a dataframe where we group by 'docID' and sum the terms.  This dataframe should\n",
    "    # have columns 'docid' and 'size' -- docid is a string, size is an int.  \n",
    "    # Hint -- when you do a sum aggregation, the column will be named 'sum(count)' and you will rename\n",
    "    # it to be 'size'\n",
    "    docSize = ??\n",
    "    \n",
    "    # Tfnormed is normalized term frequency -- it is all of the tfidf formula except for dividing by \n",
    "    #  document frequency.  In other words, it will be 1000000.0 * term-count-per-document/document-size\n",
    "    # It will have two columns (docid, tfnormed)  with types (string, float)\n",
    "    tfnormed = ??\n",
    "   \n",
    "    # To get document frequency, start with term frequency, group by term, and count the number of docids.\n",
    "    #  This dataframe should have columns (term, count)\n",
    "    df = ??\n",
    "    \n",
    "    # To get the final data frame, join tfnormed with df, and divide.\n",
    "    #  This final dataframe should have columns (term, docid, tfidf) with types (string, string, float)\n",
    "    tfidf = ??\n",
    "    \n",
    "    return tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e190dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The previous cell just defined how to create the index data frame.\n",
    "# Now we create it -- the call to cache() tells Spark to hold the data frame contents\n",
    "# in memory if possible, so the rows can be re-used.\n",
    "\n",
    "index = indexDocuments(books_directory)\n",
    "index.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75717e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just verify that the frame holds plausible data (right columns, right data types, reasonable values)\n",
    "index.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b213cafc",
   "metadata": {},
   "source": [
    "#### Relevance Calculation\n",
    "\n",
    "Relevance is a relationship between a \"query\" -- a string of words, and the index.\n",
    "The query is parsed (termified) using the same function as was used to index documents.\n",
    "Then in the Spark framework we can compute the TFIDF value for the query terms for all documents and select the top N.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca6d969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs:\n",
    "#    The query string (a string)\n",
    "#    The index (a Data Frame in the form produced by indexDocuments)\n",
    "#    Num_results (N) -- the number of search results to return\n",
    "#\n",
    "# Output:\n",
    "#    A list of N tuples of the form (docid, relevance) which are the N most relevance documents\n",
    "#     for the query, along with the relevance value for each document\n",
    "\n",
    "#  Note that this output is a Python list, not a Spark object\n",
    "\n",
    "\n",
    "def relevance(query, tfidf, num_results=5):\n",
    "    # Create a list of terms by running termify on the query string\n",
    "    queryTerms = list(termify(query))\n",
    "    \n",
    "    # These two lines create a DataFrame from the list of query terms.  The resulting \n",
    "    #  DataFrame has a single column 'term' which is a string\n",
    "    rows = map(lambda t: Row(t), queryTerms)\n",
    "    query = SQLContext(sc).createDataFrame(rows, StructType([StructField('term', StringType())]))\n",
    "    \n",
    "    # Join the query frame with the TFIDF frame on 'term'.  Group by docid and sum, which \n",
    "    #  is the total TFIDF for that term and that document.  Rename the sum of tfidf to be 'score',\n",
    "    #  and truncate score to an int (as per the formula)\n",
    "    #  The resulting DataFrame has columns (docid and score) of type (string and int)\n",
    "    j = query.join(tfidf, 'term')\\\n",
    "        .drop('term')\\\n",
    "        .groupBy(\"docid\")\\\n",
    "        .sum()\\\n",
    "        .withColumnRenamed('sum(tfidf)', 'score')\n",
    "\n",
    "    # Sort the frame in descending order of score, take the first N elements,\n",
    "    #  and format the return result to be a list of tuples of the form (docid, score)\n",
    "    return ??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cece4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test it out!  \n",
    "for query in [\"buster, whale, king, and alice the rabbit!\",\n",
    "               \"Take a whale to lunch this week!\",\n",
    "              \"What would Jesus do about that?\",\n",
    "              \"My name is Buster.  Deal with it.\",\n",
    "              \"Bodice ripper?\",\n",
    "              \"Why does it have to be sense OR sensibility, why can't it be sense AND sensibility\"\n",
    "              \"What are leaves of grass anyway?\",\n",
    "              \"??!?\"]:\n",
    "  print(query)\n",
    "  for tuple in relevance(query, index):\n",
    "        print(\"    \" + str(tuple))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
