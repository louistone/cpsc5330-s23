For this demo we will do simple processing on our documents/books.
The purpose of the demo is
  a.  To get a Map Reduce job running on AWS (Elastic Map Reduce or EMR)
  b.  To try some testing/debugging techniques, because testing in the cloud
      can be very time-consuming.  Best to debug your program as much as you
      can before moving it to AWS, then if you get errors they are probably
      related to the cloud and not your application.


Problem statement:
  * Input -- a directory containing text files (our usual text corpus)
      It will be at a shared location in S3 --   s3://cpsc5330s23/books/
  * Output -- for each document, report on (a) the longest term and its length,
      (b) the percentage of the words in the document that were filtered during
      text preprocessing

For the demo we'll use the definition of text preprocessing we used in Lab 2:
downcase then filter empty strings.  You will be doing something better in the lab.

Other rules (these are similar to those in the lab):
  * Create an S3 bucket to hold your solution
  * Create two folders to hold the code and output data
  * The mapper and reducer will be in the code subfolder, and will
    have the names demo-mapper.py and demo-reducer.py

