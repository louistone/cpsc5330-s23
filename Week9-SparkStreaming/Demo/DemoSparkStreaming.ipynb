{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "72cabaca-720e-41cb-b371-f5aa63afee89",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Structured Streaming (Based on Example from \"Spark the Definitive Guide\" Chapter 21)\n",
    "\n",
    "Files from the book's 'activity' data set are in the course repository, I moved all of them to the S3 folder **ActivityFiles** -- from there we will move a file at a time to a different folder **ActivityFilesLive** to simulate streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f3d4f7de-a0fa-4c84-af04-7ec0ddbe6a5c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### High-Level Overview\n",
    "\n",
    "**Batch Spark**\n",
    "  * Input source -- source of *records* -- usually file or files, but important concept is it's a *batch* -- process all at once, exactly once\n",
    "  * Data frame on top of the input source -- location of input plus schema\n",
    "  * Transformations on the data frame -- for example, document text to (term, doc, tfidf) records\n",
    "  * Output Sink -- files, database, DynamoDB, queue -- important concept is the whole transformed data frame is written all at once\n",
    "  \n",
    "**Streaming Spark**\n",
    "  * Input source -- source of *records* -- maybe a file, maybe a queue.  Records arrive asynchronously.\n",
    "  * Data frame on top of the input source -- location of input plus schema.  Exactly the same\n",
    "  * Transformations on the data frame -- for example, document text to (term, doc, tfidf) records\n",
    "  * Output Sink -- file, queue, (console, in-memory table)\n",
    "  \n",
    "**New Concepts**\n",
    "  * Incremental update of the data frame (!!)\n",
    "  * Trigger -- when should an incremental update happen?\n",
    "  * Output Mode -- how to update the derived data stream incrementally.  (Append, update changed records, rewrite the whole table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important setup note for running on AWS\n",
    "\n",
    "Later in this lab we will get a permissions violation when the Spark processes try to write to S3.\n",
    "\n",
    "To resolve the problem\n",
    "* When you set up the cluster, attach a key pair to it.\n",
    "* Once the cluster master node is running, SSH to that node\n",
    "* Enter this command <pre>sudo usermod -a -G hdfsadmingroup livy</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 ls s3://5330spark/ActivityFiles/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fc1a6e97-b20d-483d-8b66-75018a970ef5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "s3api put-object --bucket 5330spark --key ActivityFilesLive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stagingDirectory = \"s3://5330spark/ActivityFiles/\"\n",
    "liveDirectory = \"s3://5330spark/ActivityFilesLive/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Review -- Explore the Data Set using Static DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the input files first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d98d2071-8f4d-40f6-9281-177c23f996cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##  First step is set up a typical 'static' data frame for demonstration purposes\n",
    "##  Notice how Spark takes a directory name to mean \"all files in the directory\"\n",
    "##  Also notice how JSON as a file format works.\n",
    "\n",
    "static = spark.read.json(stagingDirectory)\n",
    "static.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "19519368-5388-4045-b0fb-c27a531ad63a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Notice that Spark inferred a schema from the header line (column names) and data values (data types)\n",
    "static.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "12838320-6037-47a8-8636-923dfc430e97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "static.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1b47d7aa-cfeb-4d8c-b9a0-25ce9cff2a7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The book saves that schema and uses it as the schema for the streaming version of this\n",
    "# data frame.  We need the explicit schema because for our streaming application we are going to \n",
    "# create the data frame before  there are any data records for inferring the schema. \n",
    "# However, it is better practice to declare the schema explicitly.\n",
    "\n",
    "# But note the issue with imposing an external schema -- if you make an error, like \n",
    "# wrong name for a field, you are in trouble!\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType, LongType\n",
    "activitySchema = StructType( [StructField(\"Arrival_Time\",LongType(),True),\n",
    "                              StructField(\"Creation_Time\",LongType(),True),\n",
    "                              StructField(\"Device\",StringType(),True),\n",
    "                              StructField(\"Index\",LongType(),True),\n",
    "                              StructField(\"Model\",StringType(),True),\n",
    "                              StructField(\"User\",StringType(),True),     ##  MISSPELLING!\n",
    "                              StructField(\"gt\",StringType(),True),\n",
    "                              StructField(\"x\",DoubleType(),True),\n",
    "                              StructField(\"y\",DoubleType(),True),\n",
    "                              StructField(\"z\",DoubleType(),True)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bc021a80-4a71-4a25-84b0-f758cbfcdd12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Just to reinforce -- read from the same file, but it give it our manually declared schema\n",
    "static = spark.read.format('json')\\\n",
    "    .options(header='false')\\\n",
    "    .options(inferSchema=False)\\\n",
    "    .schema(activitySchema)\\\n",
    "    .load(stagingDirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9cafd2a1-a5fb-46f5-8e51-e8ded6e2b205",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "static.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "31ba8eab-84d4-4f7c-8e59-e96c31c19a10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "static.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No data dictionary!    Take a look at device, index, model, user, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This just establishes a \"query\" against the static data set\n",
    "activityCounts = static.groupBy(\"gt\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we run the query, reading and processing the whole batch\n",
    "activityCounts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  For later analysis we will clean up the data frame.\n",
    "#   Just use the fields gt, model, z, and creation_time\n",
    "#   Filter null values\n",
    "#   Make the names nicer\n",
    "\n",
    "static = spark.read.format('json')\\\n",
    "    .options(header='false')\\\n",
    "    .options(inferSchema=False)\\\n",
    "    .schema(activitySchema)\\\n",
    "    .load(stagingDirectory)\\\n",
    "    .select(\"gt\", \"Creation_Time\", \"device\", \"z\")\\\n",
    "    .withColumnRenamed(\"gt\", \"activity\")\\\n",
    "    .withColumnRenamed(\"Creation_Time\", \"creation_time\")\\\n",
    "    .filter(\"gt != 'null'\")\\\n",
    "    .filter(\"device != 'null'\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static.groupBy('activity').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static.select('activity', 'z').groupby('activity').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now to the World of Streaming!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "be1f71aa-bc87-4e1b-ae1b-ec282512a7e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  Create exactly the same data frame, except a streaming version.  Notice that it reads from activity-live,\n",
    "#  which is empty at the moment.\n",
    "#  Notice that this is essentially the same as creating the static data frame except \n",
    "#    readStream instead of read\n",
    "#    the 'maxFilesPerTrigger'\n",
    "\n",
    "streaming = spark.readStream\\\n",
    "    .schema(activitySchema)\\\n",
    "    .option(\"maxFilesPerTrigger\",1)\\\n",
    "    .json(liveDirectory)\\\n",
    "    .select(\"gt\", \"Creation_Time\", \"device\", \"z\")\\\n",
    "    .withColumnRenamed(\"gt\", \"activity\")\\\n",
    "    .withColumnRenamed(\"Creation_Time\", \"creation_time\")\\\n",
    "    .filter(\"gt != 'null'\")\\\n",
    "    .filter(\"device != 'null'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's a DataFrame.  \n",
    "type(streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  So let's take a look!\n",
    "streaming.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "03a255d7-f64c-4580-85c7-b3dc148941e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Notice this sets up a data frame based on a streaming data frame.\n",
    "# Exact same syntax as the static version.\n",
    "\n",
    "activityCounts = streaming.groupBy(\"activity\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "741a22dc-f485-46f0-b6bb-ddd012c190fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# But how do we then extract information from it?\n",
    "activityCounts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(activityCounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f397623b-68ad-4b94-99aa-d0bae64d2bf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Advice from the book since we're running on a single worker.\n",
    "# Lots of partitions will be harmful if there's only a few workers!\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0ba1af5a-29f6-4515-b1b3-ff69e6505afa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Here is our fundamental way of getting information to our streaming \"consumers\"\n",
    "#    We already saw the queryName method -- lets us do SQL operations on a query name \n",
    "#        .writeStream references a stream that will handle incremental changes to the query\n",
    "#           -- notice that writeStream is the receiver for all the subsequent calls\n",
    "#        .format says that the 'consumer' of the stream (the 'sink') is an in-memory table\n",
    "#        .queryName points to the in-memory location of our query results\n",
    "#        .outputMode means rewrite the whole table every time its contents changes\n",
    "#        .start begins a process of monitoring the streaming data frame for changes\n",
    "\n",
    "activityQuery = activityCounts\\\n",
    "    .writeStream\\\n",
    "    .format(\"memory\")\\\n",
    "    .queryName(\"activity_counts_memory\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Careful when running streams in a notebook.\n",
    "\n",
    "The book says to do \n",
    "<pre>\n",
    "activityQuery.awaitTermination()\n",
    "</pre>\n",
    "But that causes the query to hang when run in a notebook.\n",
    "\n",
    "We need to be careful to do \n",
    "<pre>\n",
    "anyQuery.stop()\n",
    "</pre>\n",
    "instead when we are finished with the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5664b9a7-67a5-4907-9cbf-d1909c6d9368",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy way to stop all streams\n",
    "def stop_all_streams():\n",
    "    for s in spark.streams.active:\n",
    "        s.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "80029806-ece5-4799-9d42-1714356b6a98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reference the query name above\n",
    "spark.sql(\"SELECT * FROM activity_counts_memory\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 cp s3://5330spark/ActivityFiles/0.json s3://5330spark/ActivityFilesLive/0.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 ls s3://5330spark/ActivityFilesLive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "026b7608-f7e7-40ec-af3d-42e55d5775ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM activity_counts_memory\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fecf0107-fdc1-48ad-b5ab-1c6bb10304f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 cp s3://5330spark/ActivityFiles/1.json s3://5330spark/ActivityFilesLive/1.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "11dc3770-99c8-4764-94ef-898a145a03ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM activity_counts_memory\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1e665581-9562-461f-aea0-5d8585eb7005",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create a different query on the same streaming data frame, this\n",
    "with some more complexity like selecting certain rows and removing a column, but does no aggregation.\n",
    "Notice the output mode of append.  It will just add new records to the end of the\n",
    "query.   This makes sense since as you add more records to the stream, the output\n",
    "stream will just increase in rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_all_streams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ad20ffce-f4bd-4e7e-96ff-dc844b72e9c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Notice these restrictions are in addition to restrictions on \n",
    "# the 'streaming' dataframe\n",
    "\n",
    "simpleTransform = streaming\\\n",
    "    .select(\"activity\", \"device\")\\\n",
    "    .where(\"activity not like '%stairs%'\")\\\n",
    "    .where(\"device = 'nexus4_2'\")\\\n",
    "    .writeStream\\\n",
    "    .queryName(\"simple_transform\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3541ec91-18a4-494e-b32a-2a970ea6ef78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what's in the live directory at the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fe75ad0a-de1b-400c-8656-594cedffe7c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM simple_transform\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d6b6c349-e6bb-4fc3-aa81-701d5118242f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 cp s3://5330spark/ActivityFiles/0.json s3://5330spark/ActivityFilesLive/0.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM simple_transform\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "18e14a3c-529b-4484-93e5-5e88ec191dfa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM simple_transform limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 cp s3://5330spark/ActivityFiles/1.json s3://5330spark/ActivityFilesLive/1.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3a9169d6-f8c6-467e-a080-d353b6593f66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT count(*) FROM simple_transform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "240befc5-b528-4a15-903a-c7adbf4cfa16",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Third example, aggregations\n",
    "```\n",
    "The cube function “takes a list of columns and applies aggregate expressions to all possible combinations of the grouping columns”.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "92b64fd2-f980-443c-a1c5-969471ee0d3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "static.cube(\"gt\", \"device\").avg().filter(\"gt != 'null' and device != 'null'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_all_streams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files 0 and 1 should be in the live directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "83638a87-0058-4224-bfdd-1f2af3be1d29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deviceModelStats = streaming\\\n",
    "    .cube(\"gt\", \"device\")\\\n",
    "    .avg()\\\n",
    "    .filter(\"device != 'null'\")\\\n",
    "    .writeStream\\\n",
    "    .queryName(\"device_stats\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "38ed14ea-7da2-4238-a691-c0b4e20d758c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM device_stats\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 cp s3://5330spark/ActivityFiles/4.json s3://5330spark/ActivityFilesLive/4.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM device_stats\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4ffcad36-73b9-46fd-8c0f-cb3a6d7ba79a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Demonstrates joining a streaming dataframe (deviceModelStats) with a static stream.\n",
    "Here the static stream is the historical average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e6bc7b94-da72-4725-ac73-0dd334e2b2fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Static historical average for x, y, z taken from all files\n",
    "historicalAgg = static\\\n",
    "    .select('Device', 'gt', 'z')\\\n",
    "    .withColumnRenamed('Device', 'device')\\\n",
    "    .withColumnRenamed('gt', 'activity')\n",
    "    .cube('device', 'activity')\\\n",
    "    .avg()\\\n",
    "    .filter(\"device != 'null'\")\\\n",
    "    .filter(\"gt != 'null'\")\\\n",
    "    .withColumnRenamed('avg(z)', 'historical_z')\n",
    "\n",
    "historicalAgg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5dfd8a31-ea92-452f-90dd-0f29af8898a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deviceModelCleaned = streaming\\\n",
    "    .select(\"device\", \"activity\", \"z\")\\\n",
    "    .cube(\"gt\", \"device\").avg()\\\n",
    "    .withColumnRenamed(\"avg(z)\", \"average_z\")\n",
    "\n",
    "deviceModelJoined = deviceModelCleaned\\\n",
    "  .join(historicalAgg, [\"activity\", \"device\"])\\\n",
    "  .writeStream\\\n",
    "  .queryName(\"device_model_joined\")\\\n",
    "  .format(\"memory\")\\\n",
    "  .outputMode(\"complete\")\\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the usual operations for a streaming query, just\n",
    "# demonstrating they don't need to be method chained\n",
    "\n",
    "ws = deviceModelJoined.writeStream\n",
    "ws.queryName(\"device_model_joined\")\n",
    "ws.format('memory')\n",
    "ws.outputMode('complete')\n",
    "ws.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4eff7b1d-67b8-4c0b-ba31-b819a79a8440",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from device_model_joined\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "78cb7c72-e4c5-47d8-a1dd-a69354c03cc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 cp s3://5330spark/ActivityFiles/5.json s3://5330spark/ActivityFilesLive/5.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b43e26c3-8ffe-4905-85a0-56344c25c04f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from device_model_joined\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "732eb91a-0fb7-46ea-b4a9-b72a9300a8be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stop_all_streams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empty the live folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a58901ef-7df3-4d6a-b4f1-07c727e2fa79",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Experiment with Update and Append Modes, Both in Memory and With an S3 Sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Append in memory -- suppose we are just \"cleaning\"the data set, \n",
    "##  keep just gt, device, and Creation_Time -- filter out null gt and device,\n",
    "##  rename the column\n",
    "\n",
    "streaming = spark.readStream.\\\n",
    "  schema(activitySchema).\\\n",
    "  option(\"maxFilesPerTrigger\",1).\\\n",
    "  json(liveDirectory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleAppend = streaming\\\n",
    "    .select(\"gt\", \"device\", \"Creation_Time\")\\\n",
    "    .withColumnRenamed(\"Creation_Time\", \"creation_time\")\\\n",
    "    .withColumnRenamed(\"gt\", \"activity\")\\\n",
    "    .filter(\"activity != 'null'\")\\\n",
    "    .filter(\"device != 'null'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as before, but different output mode\n",
    "simpleAppend\\\n",
    "    .writeStream\\\n",
    "    .queryName(\"simple_append\")\\\n",
    "    .format('memory')\\\n",
    "    .outputMode('append')\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nothing to see here :-)\n",
    "# Something different is happening, new rows are being appended to the table, but we can't see it\n",
    "\n",
    "spark.sql(\"select * from simple_append\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 cp s3://5330spark/ActivityFiles/0.json s3://5330spark/ActivityFilesLive/0.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  For update, number of events per gt.  This really should update as we get new events.\n",
    "simpleUpdate = streaming\\\n",
    "    .select(\"activity\")\\\n",
    "    .filter(\"activity != 'null'\")\\\n",
    "    .groupBy(\"activity\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleUpdate\\\n",
    "    .writeStream\\\n",
    "    .queryName(\"simple_update\")\\\n",
    "    .format('memory')\\\n",
    "    .outputMode('update')\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from simple_update\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 cp s3://5330spark/ActivityFiles/7.json s3://5330spark/ActivityFilesLive/7.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Look carefully -- this is unexpected.  What is going on??\n",
    "spark.sql(\"select * from simple_update\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Using S3 as a Stream Sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_all_streams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empty the live directory again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Try output mode of complete -- completely replace the data table.\n",
    "##  Can streaming completely replace the files it has already written?\n",
    "\n",
    "countByGt = streaming\\\n",
    "    .select(\"gt\")\\\n",
    "    .filter(\"gt != 'null'\")\\\n",
    "    .groupBy(\"gt\")\\\n",
    "    .count()\n",
    "\n",
    "countByGt\\\n",
    "    .writeStream\\\n",
    "    .format(\"text\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .option(\"path\", \"s3://5330spark/Output/Complete/\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try append with an S3 Sink -- just select columns and filter (no aggregation)\n",
    "We would expect to see files added to the output folder as we add more files to the live input director"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_all_streams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appendGtEvents = streaming\\\n",
    "    .select(\"gt\", \"Device\")\\\n",
    "    .filter(\"gt != 'null'\")\\\n",
    "    .filter(\"Device != 'null'\")\n",
    "\n",
    "appendGtEvents.writeStream\\\n",
    "    .format(\"csv\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .option(\"path\", \"s3://5330spark/Output/Complete/\")\\\n",
    "    .option(\"checkpointLocation\", \"s3://5330spark/Output/Checkpoints/\")\\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 cp s3://5330spark/ActivityFiles/7.json s3://5330spark/ActivityFilesLive/7.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 ls s3://5330spark/Output/Complete/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we do an append output mode with a grouping query and an S3 sink?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appendGrouped = streaming\\\n",
    "    .select(\"gt\", \"z\")\\\n",
    "    .groupBy(\"gt\")\\\n",
    "    .avg()\n",
    "\n",
    "appendGrouped\\\n",
    "    .writeStream.format(\"csv\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .option(\"checkpointLocation\", \"s3://5330spark/Output/Checkpoints/\")\\\n",
    "    .option(\"path\", \"s3://5330spark/Output/Complete/\")\\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what do you suppose the problem is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary -- Spark Streaming\n",
    "\n",
    "Philosophy\n",
    "    * Streaming processing code should be as close as possible to static processing code\n",
    "    * But streaming process looks at new records incrementally as they \"appear\"\n",
    "    \n",
    "* Streaming Data Frame\n",
    "    * Input source -- files in a directory, Kafka queue\n",
    "    * Schema\n",
    "    * Trigger -- file being added, message received\n",
    "    \n",
    "* Query\n",
    "  * Based on a streaming data frame\n",
    "  * Supports all data frame operation (select, project, aggregations, joins)\n",
    "  \n",
    "* Write Streams\n",
    "  * Based on a streaming query\n",
    "  * Output mode -- complete, append, or update\n",
    "      * Complete, replace all records with new records\n",
    "      * Append, add new records to the old records\n",
    "      * Update, add only records that have changed\n",
    "  * Sink -- in memory, folder, Kafka queue\n",
    "  * Starts and Stops\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "StreamingFromBook",
   "notebookOrigID": 2252422431347658,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "name": "StreamingFromBook",
  "notebookId": 3201503076658016
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
